@misc{AlexanderMordvintsevChristopherOlah2015,
abstract = {Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer Update ...},
author = {{Alexander Mordvintsev, Christopher Olah}, Mike Tyka},
booktitle = {Google AI},
title = {{Google AI Blog: Inceptionism: Going Deeper into Neural Networks}},
url = {https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
year = {2015}
}
@article{Ramanujan2019,
abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 trained on ImageNet. Not only do these "untrained subnetworks" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an "untrained subnetwork" approaches a network with learned weights in accuracy.},
archivePrefix = {arXiv},
arxivId = {1911.13299},
author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
eprint = {1911.13299},
file = {:C$\backslash$:/Users/raficohen/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramanujan et al. - Unknown - What's Hidden in a Randomly Weighted Neural Network.pdf:pdf},
title = {{What's Hidden in a Randomly Weighted Neural Network?}},
url = {http://arxiv.org/abs/1911.13299},
year = {2019}
}
@article{Yin2019,
abstract = {We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We 'invert' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance -- (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.},
archivePrefix = {arXiv},
arxivId = {1912.08795},
author = {Yin, Hongxu and Molchanov, Pavlo and Li, Zhizhong and Alvarez, Jose M and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan},
eprint = {1912.08795},
file = {:C$\backslash$:/Users/raficohen/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - Unknown - Dreaming to Distill Data-free Knowledge Transfer via DeepInversion.pdf:pdf},
title = {{Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion}},
url = {http://arxiv.org/abs/1912.08795},
year = {2019}
}
