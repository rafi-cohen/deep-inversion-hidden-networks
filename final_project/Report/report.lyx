#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Final Project Report
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reg}{\mathcal{R}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\loss}{\mathcal{L}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rtv}{\reg_{\text{TV}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rnorm}{\reg_{\ell_{2}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rprior}{\reg_{\text{prior}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rfeature}{\reg_{\text{feature}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\atv}{\alpha_{\text{tv}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\anorm}{\alpha_{\ell_{2}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\afeature}{\alpha_{\text{feature}}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\xhat}{\hat{x}}
\end_inset


\end_layout

\begin_layout Section
Abstract
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Deep Dream
\end_layout

\begin_layout Standard
Deep Dream 
\begin_inset CommandInset citation
LatexCommand cite
key "AlexanderMordvintsevChristopherOlah2015"
literal "false"

\end_inset

 is a popular method that have been proposed to visualize what a deep network
 expects to see in an image.
 Given a pretrained classification CNN 
\begin_inset Formula $\mathcal{N}$
\end_inset

 and an input image 
\begin_inset Formula $\hat{x}$
\end_inset

, the method forwards 
\begin_inset Formula $\hat{x}$
\end_inset

 through 
\begin_inset Formula $\mathcal{N}$
\end_inset

 up to a certain pre-chosen layer 
\begin_inset Formula $\mathcal{\ell}$
\end_inset

, and then tries to maximize the activations of 
\begin_inset Formula $\mathcal{\ell}$
\end_inset

 by using gradient ascent on the 
\begin_inset Formula $\mathcal{\ell}_{2}$
\end_inset

 norm of these activations.
 During this process 
\begin_inset Formula $\mathcal{N}$
\end_inset

 is kept fixed while the input image is gradually transformed to yield high
 output responses for certain classes.
 The features of the classes depicted in the output image depend on 
\begin_inset Formula $\hat{x}$
\end_inset

 and on the visual knowledge stored in 
\begin_inset Formula $\mathcal{N}$
\end_inset

.
 For example - if part of 
\begin_inset Formula $\hat{x}$
\end_inset

 resembles something that 
\begin_inset Formula $\mathcal{N}$
\end_inset

 interprets even slightly as a dog, then Deep Dream will emphasize and enhance
 these features to look more and more like a dog.
\end_layout

\begin_layout Standard
Apart from the above method, which is class neutral, another method is mentioned
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "AlexanderMordvintsevChristopherOlah2015"
literal "false"

\end_inset

 which allows enhancing an input image in a way which would elicit a 
\series bold
particular interpretation
\series default
.
 Starting from an input image 
\begin_inset Formula $\hat{x}$
\end_inset

 full of random noise, a target class 
\begin_inset Formula $y$
\end_inset

 and a pretrained classification network 
\begin_inset Formula $\mathcal{N}$
\end_inset

, we forward 
\begin_inset Formula $\hat{x}$
\end_inset

 through 
\begin_inset Formula $\mathcal{N}$
\end_inset

 and then gradually tweak the image towards what 
\begin_inset Formula $\mathcal{N}$
\end_inset

 considers as 
\begin_inset Formula $y$
\end_inset

.
 This optimization process can be expressed as solving an optimization problem
 of the form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{\hat{x}}\loss\left(\hat{x},y\right)+\reg\left(\hat{x}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{L}\left(\cdot\right)$
\end_inset

 is a classification loss (e.g.
 standard softmax cross-entropy), and 
\begin_inset Formula $\mathcal{R}\left(\cdot\right)$
\end_inset

 is an image regularization term to improve 
\begin_inset Formula $\hat{x}$
\end_inset

’s visual quality.
\end_layout

\begin_layout Standard
In order to steer the generated images away from unrealistic images that
 are classified as 
\begin_inset Formula $y$
\end_inset

 but posses no discernible visual information, Deep Dream proposes to add
 an image prior term 
\begin_inset Formula $\rprior$
\end_inset

 which penalizes the total variation and 
\begin_inset Formula $\mathcal{\ell}_{2}$
\end_inset

 norm of 
\begin_inset Formula $\hat{x}$
\end_inset

 during the generation process.
 This enforces the output image to have similar statistics to natural images,
 for example by making neighboring pixels more correlated.
\end_layout

\begin_layout Subsection
Deep Inversion
\end_layout

\begin_layout Standard
Following in Deep Dreams's footsteps, Deep Inversion 
\begin_inset CommandInset citation
LatexCommand cite
key "Yin2019"
literal "false"

\end_inset

 (DI) is an improved method for reconstructing class-conditional images
 from a pretrained classification model.
\end_layout

\begin_layout Standard
Despite the contribution of the regularization term proposed by Deep Dream,
 images generated by this method still lack an overlap in their distribution
 with natural (or original training) images, and thus lead to unsatisfactory
 results for uses such as knowledge distillation.
\end_layout

\begin_layout Standard
The creators of DI propose to solve this problem by introducing a new term
 to the above optimization objective: a feature distribution regularization
 term called 
\begin_inset Formula $\rfeature$
\end_inset

 .
 By relying on the hidden data stored within the batch normalization 
\begin_inset CommandInset citation
LatexCommand cite
key "Ioffe2015"
literal "false"

\end_inset

 layers of the pretrained model, this term constrains the synthesized images
 to the same distribution as the model's original training images.
 This greatly improves the quality of the reconstructed images.
 
\end_layout

\begin_layout Standard
Apart from improving the visual quality of the reconstructed images, Yin
 et al.
 also manage to improve the diversity of the synthesized images by introducing
 Adaptive Deep Inversion (ADI).
 This improved method adds an additional 'student' model to the original
 pretrained 'teacher' model, and then tries to maximize the Jensen-Shannon
 divergence between the logits of the two models to enforce exploring new
 synthesis directions.
 However, since the diversity of the generated images is not an important
 factor in our project we have decided not to include the improvement introduced
 by ADI and to use DI exclusively.
\end_layout

\begin_layout Subsection
Hidden Networks
\end_layout

\begin_layout Standard
Traditionally, deep learning involves initializing a network with random
 weights, and then learning and optimizing the weights to achieve good performan
ce for some given task.
 In their paper, Ramanujan et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ramanujan2019"
literal "false"

\end_inset

 propose an interesting take on learning.
 They show that large networks which are initialized randomly using a properly
 scaled distribution (e.g.
 Kaiming 
\begin_inset Note Note
status open

\begin_layout Plain Layout
reference
\end_layout

\end_inset

) often contain smaller subnetworks that perform quite well.
 Not only that, but they also suggest the edge-popup algorithm - an algorithm
 that attempts to find such subnetworks by assigning scores to the edges
 (weights) of the network, and choosing the edges with the best scores in
 each layer.
 The algorithm learns the scores without ever modifying the weights of the
 network.
 In their experiments, they were able to find a subnetwork of a randomly
 initialized Wide ResNet-50 
\begin_inset Note Note
status open

\begin_layout Plain Layout
reference
\end_layout

\end_inset

 network that is only 30% of its size, and performs as well as a trained
 ResNet-34.
\end_layout

\begin_layout Section
Methods 
\end_layout

\begin_layout Subsection
Deep Inversion
\end_layout

\begin_layout Standard
Recall that the purpose of Deep Inversion is to 'invert' images from a pretraine
d CNN.
 Given a randomly initialized input 
\begin_inset Formula $\hat{x}\in\mathbb{R}^{H\times W\times C}$
\end_inset

 (
\begin_inset Formula $H,W,C$
\end_inset

 being the height, width, and number of color channels), and an arbitrary
 target label 
\begin_inset Formula $y$
\end_inset

, the image synthesis process can be expressed as solving the following
 optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\min_{\hat{x}}\loss\left(\hat{x},y\right)+\reg\left(\hat{x}\right)\label{eq:loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{L}\left(\cdot\right)$
\end_inset

 is a classification loss such as the standard softmax cross-entropy, and
 
\begin_inset Formula $\mathcal{R}\left(\cdot\right)$
\end_inset

 is an image regularization term to improve 
\begin_inset Formula $\hat{x}$
\end_inset

’s visual quality.
\end_layout

\begin_layout Subsubsection
Regularization Terms
\begin_inset CommandInset label
LatexCommand label
name "Method:Regularizations"

\end_inset


\end_layout

\begin_layout Subsubsection*
Prior Regularization
\end_layout

\begin_layout Standard
Deep Dream 
\begin_inset CommandInset citation
LatexCommand cite
key "AlexanderMordvintsevChristopherOlah2015"
literal "false"

\end_inset

 suggests the following regularization term:
\begin_inset Formula 
\[
\rprior\left(\hat{x}\right)=\atv\rtv\left(\hat{x}\right)+\anorm\rnorm\left(\hat{x}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The first term, 
\begin_inset Formula $\rtv$
\end_inset

, penalizes the total variation of 
\begin_inset Formula $\hat{x}$
\end_inset

, thus promoting correlation between nearby pixels.
 The term is defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\rtv(\hat{x})=\lVert\hat{x}-\hat{x}_{H}\rVert_{2}+\lVert\hat{x}-\hat{x}_{V}\rVert_{2}+\lVert\hat{x}-\hat{x}_{D_{1}}\rVert_{2}+\lVert\hat{x}-\hat{x}_{D_{2}}\rVert_{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{x}_{H},\hat{x}_{V},\hat{x}_{D_{1}},\hat{x}_{D_{2}}$
\end_inset

 are horizontal, vertical and diagonal shifted variants of 
\begin_inset Formula $\hat{x}$
\end_inset

, all by a single pixel (note that 
\begin_inset Formula $\hat{x}_{D_{1}}$
\end_inset

 and 
\begin_inset Formula $\hat{x}_{D_{2}}$
\end_inset

 represent two 
\bar under
different
\bar default
 diagonal shifts).
\end_layout

\begin_layout Standard
The second term, 
\begin_inset Formula $\rnorm$
\end_inset

, is a simpler term which penalizes the 
\begin_inset Formula $\ell_{2}$
\end_inset

 norm of 
\begin_inset Formula $\xhat$
\end_inset

, and is defined as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\rnorm(\xhat)=\lVert\xhat\rVert_{2}
\]

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "Mahendran2014"
literal "false"

\end_inset

 it is claimed that this term encourages the range of the values in the
 image to stay within a target interval instead of diverging.
\end_layout

\begin_layout Standard
Note that the magnitude of the above two regularization terms (as they are
 described in the paper) grows linearly with the batch size, which makes
 it impossible to run hyper-parameter tuning algorithms with different batch
 sizes.
 Thus, in order to make these terms batch-size independent we decided to
 normalize them by dividing them by the batch size.
\end_layout

\begin_layout Standard
Lastly, 
\begin_inset Formula $\atv,\anorm$
\end_inset

 are the scaling factors of the above regularization terms.
\end_layout

\begin_layout Subsubsection*
Feature Regularization
\end_layout

\begin_layout Standard
The main advancement proposed by Deep Inversion (when compared with Deep
 Dream) is the introduction of a new feature regularization term denoted
 as 
\begin_inset Formula $\rfeature$
\end_inset

.
 
\end_layout

\begin_layout Standard
Yin et al.
 claim that the regularization terms proposed by Deep Dream provide little
 guidance for manipulating 
\begin_inset Formula $\xhat$
\end_inset

 to contain both low and high level features that are similar to real training
 images.
 To effectively enforce feature similarities at all levels, the authors
 of DI propose to minimize the distance between the feature map statistics
 of 
\begin_inset Formula $\xhat$
\end_inset

 and that of the original dataset 
\begin_inset Formula $\mathcal{X}$
\end_inset

 on which the pretrained model 
\begin_inset Formula $\mathcal{N}$
\end_inset

 was trained.
\end_layout

\begin_layout Standard
Assuming that the feature statistics follow a Gaussian distribution across
 batches (
\begin_inset Formula $\sim\mathcal{N}(\mu,\sigma^{2}))$
\end_inset

, 
\begin_inset Formula $\rfeature$
\end_inset

 is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\rfeature(\xhat)=\sum_{l}\left\Vert \mu_{l}\left(\hat{x}\right)-\mathbb{E}(\mu_{l}(x)|\mathcal{\mathcal{X}})\right\Vert _{2}+\sum_{l}\left\Vert \sigma_{l}^{2}\left(\hat{x}\right)-\mathbb{E}(\text{\ensuremath{\sigma_{l}^{2}}}(x)|\mathcal{\mathcal{X}})\right\Vert _{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu_{l}\left(\hat{x}\right)$
\end_inset

 and 
\begin_inset Formula $\sigma_{l}^{2}\left(\hat{x}\right)$
\end_inset

 are the batch-wise mean and variance estimates of feature maps corresponding
 to the 
\begin_inset Formula $l$
\end_inset

-th convolutional layer of 
\begin_inset Formula $\mathcal{N}$
\end_inset

.
\end_layout

\begin_layout Standard
Obtaining 
\begin_inset Formula $\mathbb{E}(\mu_{l}(x)|\mathcal{\mathcal{X}})$
\end_inset

 and 
\begin_inset Formula $\mathbb{E}(\text{\ensuremath{\sigma_{l}^{2}}}(x)|\mathcal{\mathcal{X}})$
\end_inset

 might seem problematic since the original dataset 
\begin_inset Formula $\mathcal{X}$
\end_inset

 might not be available.
 Instead, the authors of DI suggest an intriguing solution: Using the running
 average statistics stored in the batchnorm layers 
\begin_inset CommandInset citation
LatexCommand cite
key "Ioffe2015"
literal "false"

\end_inset

 of the pretrained model.
 These layers implicitly capture the channel-wise means and variances during
 training, which allows the estimation of the above expectations by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}(\mu_{l}(x)|\mathcal{\mathcal{X}})\simeq BN_{l}(running\_mean)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}(\sigma_{l}^{2}(x)|\mathcal{\mathcal{X}})\simeq BN_{l}(running\_variance)
\]

\end_inset


\end_layout

\begin_layout Standard
Since the introduction of Batch Normalization in 
\begin_inset CommandInset citation
LatexCommand cite
key "Ioffe2015"
literal "false"

\end_inset

 BN layers have become widely adopted and can be found today in almost every
 well performing classification model.
 This makes it possible to use Deep Inversion with almost any state of the
 art classification model which exists today.
\end_layout

\begin_layout Standard
Together with 
\begin_inset Formula $\rfeature$
\end_inset

, the entire regularization term 
\begin_inset Formula $\mathcal{R}\left(\cdot\right)$
\end_inset

 from equation (1) can be expressed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{R}_{DI}(\xhat)=\rprior(\xhat)+\alpha_{f}\rfeature(\xhat)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\alpha_{f}$
\end_inset

 is a scaling factor for 
\begin_inset Formula $\rfeature$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Image Refinement Techniques
\end_layout

\begin_layout Standard
Deep Inversion incorporates two techniques that were first used in Deep
 Dream to improve the quality of the reconstructed images:
\end_layout

\begin_layout Subsubsection*
Image Clipping
\end_layout

\begin_layout Standard
Before each forward pass the synthesized images are clipped so that they
 would conform to the mean and variance of the original training set 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 Maintaining the correct pixel range during training encourages the synthesis
 process to produce valid images (whose pixels must all be in the legal
 range).
\end_layout

\begin_layout Standard
For a given synthesized image 
\begin_inset Formula $\xhat$
\end_inset

 during training, the clipping process is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\xhat=\min(\max(\xhat,-m/s),(1-m)/s)
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $s$
\end_inset

 are the channel-wise RGB mean and standard deviation of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
\end_layout

\begin_layout Standard
Our empirical testing shows that neglecting to clip the images before each
 forward pass during the training process results in many dark/bright zones
 in the produced images.
 This happens since without repeated clipping many of the pixels diverge
 during training outside of the legal range, and thus must be clipped at
 the end of the reconstruction process before the resulting images can be
 converted to PNG format.
 Our hypothesis is that repeated clipping during the training process encourages
 these pixels over and over again to converge to legal values within the
 legal range.
\end_layout

\begin_layout Subsubsection*
Random Jitter
\end_layout

\begin_layout Standard
Before each forward pass each synthesized image is temporarily offset by
 a random jitter of up to 
\begin_inset Formula $j$
\end_inset

 pixels (where 
\begin_inset Formula $j$
\end_inset

 is a hyper-parameter).
 Our hypothesis is that these shifts introduce randomness into the gradient
 descent algorithm, which slows down convergence but also helps produce
 
\begin_inset Quotes eld
\end_inset

smoother
\begin_inset Quotes erd
\end_inset

 images.
\end_layout

\begin_layout Subsubsection*
Random Flipping
\end_layout

\begin_layout Standard
Before each forward pass each synthesized image is bound for a temporary
 horizontal flip at a probability of 
\begin_inset Formula $p_{f}$
\end_inset

 (where 
\begin_inset Formula $p_{f}$
\end_inset

 is a hyper-parameter).
 The logic behind these flips is that the pretrained network 
\begin_inset Formula $\mathcal{N}$
\end_inset

 should be able to detect objects in real images regardless of whether they
 are flipped or not.
 Thus, flipping the synthesized images between iterations helps instill
 this property in the synthesized images as well with the intention of improving
 their quality.
\end_layout

\begin_layout Subsubsection
Accelerated Reconstruction Techniques
\end_layout

\begin_layout Standard
The authors of DI also recommend the following two methods to speed up the
 image reconstruction process:
\end_layout

\begin_layout Subsubsection*
Multi-Resolution Synthesis
\end_layout

\begin_layout Standard
The authors of DI found out that they can speed up the synthesis process
 by employing a multi-resolution optimization scheme.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $224\times224$
\end_inset

 ImageNet-sized images for example, the scheme begins by first optimizing
 an input of 
\begin_inset Formula $112\times112$
\end_inset

 images for a few thousand iterations, then the images are upsampled to
 
\begin_inset Formula $224\times224$
\end_inset

 via nearest-neighbor interpolation, and then they are optimized for a few
 extra thousand iterations.
 Overall this whole process requires significantly less iterations compared
 with the original method, and most of these iterations are also faster
 since they optimize much smaller images.
\end_layout

\begin_layout Subsubsection*
Automatic Mixed Precision (AMP)
\end_layout

\begin_layout Standard
To speed up the training process and reduce GPU memory consumption the authors
 of DI recommend training using half-precision floating point (FP16) via
 the NVIDIA Apex library
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
https://github.com/NVIDIA/apex
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Hidden Networks
\end_layout

\begin_layout Standard
The method for finding well performing subnetworks which are hidden in randomly
 initialized subnetworks is based on the work of Zhou et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhou2019"
literal "false"

\end_inset

.
 In their algorithm, they essentially replace each weight 
\begin_inset Formula $w$
\end_inset

 of the network with 
\begin_inset Formula $\tilde{w}=wX$
\end_inset

, where 
\begin_inset Formula $X$
\end_inset

 is a 
\begin_inset Formula $\text{Bernoulli}\left(p\right)$
\end_inset

 random variable, i.e.
 
\begin_inset Formula $X=\begin{cases}
1 & \text{with probability }p\\
0 & \text{with probability }1-p
\end{cases}$
\end_inset

.
 The value of 
\begin_inset Formula $p$
\end_inset

 for each weight is then learned using SGD.
 These random variables can be thought of as binary masks for the weights,
 and they effectively define the subnetwork.
 Masks corresponding to subnetworks that perform better than the original
 untrained network are referred to as 
\begin_inset Quotes eld
\end_inset

Supermasks
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Ramanujan et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ramanujan2019"
literal "false"

\end_inset

 build on this idea, but claim that the stochasticity of the method proposed
 by Zhou et al.
 limits the performance of the final subnetworks.
 Instead, Ramanujan et al.
 suggest assigning a popup score 
\begin_inset Formula $s$
\end_inset

 to each weight 
\begin_inset Formula $w$
\end_inset

 in the network, and then choosing the mask such that only the weights with
 the top-
\begin_inset Formula $k\%$
\end_inset

 scores of each layer are selected (
\begin_inset Formula $k$
\end_inset

 is some predefined value, and can be different for each layer).
\end_layout

\begin_layout Standard
But how would the score be chosen? First, the scores are initialized randomly
 using a Kaiming uniform distribution 
\begin_inset CommandInset citation
LatexCommand cite
key "He2015"
literal "false"

\end_inset

.
 Then, suppose we are looking at weight 
\begin_inset Formula $w_{uv}$
\end_inset

, which connects node 
\begin_inset Formula $u$
\end_inset

 to node 
\begin_inset Formula $v$
\end_inset

 in the network.
 The idea is to perform backpropagation on the loss function, look at the
 gradients of 
\begin_inset Formula $v$
\end_inset

's input and the weighted output of node 
\begin_inset Formula $u$
\end_inset

, and increase 
\begin_inset Formula $s_{uv}$
\end_inset

 if the negative gradient of 
\begin_inset Formula $v$
\end_inset

's input is aligned with 
\begin_inset Formula $u$
\end_inset

's weighted output.
 Essentially, this means we check how the loss 
\begin_inset Quotes eld
\end_inset

wants
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $v$
\end_inset

's input to change, and increase the score of 
\begin_inset Formula $w_{uv}$
\end_inset

 if 
\begin_inset Formula $u$
\end_inset

's weighted output takes it there.
 This translates into the following update rule, which can be performed
 using SGD
\begin_inset Formula 
\[
s_{uv}\leftarrow s_{uv}-\alpha\frac{\partial\mathcal{L}}{\partial\mathcal{I}_{v}}\mathcal{Z}_{u}w_{uv}
\]

\end_inset


\end_layout

\begin_layout Standard
where:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

 is the learning rate
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{L}$
\end_inset

 is the loss function
\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{I}_{v}$
\end_inset

 is the input of node 
\begin_inset Formula $v$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\mathcal{Z}_{u}$
\end_inset

 is the output of node 
\begin_inset Formula $u$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\mathcal{Z}_{u}=\sigma\left(\mathcal{I}_{u}\right)$
\end_inset

, where 
\begin_inset Formula $\sigma$
\end_inset

 is some activation function (e.g.
 ReLU)
\end_layout

\end_deeper
\begin_layout Standard
There is one important caveat here.
 The loss function is calculated on the output of a subnetwork of the original
 network.
 This can be done by using a mask similar to that used by Zhou et al., except
 here instead of a 
\begin_inset Formula $\text{Bernoulli}\left(p\right)$
\end_inset

 random variable, the mask is deterministic and denoted by 
\begin_inset Formula $h\left(s_{uv}\right)$
\end_inset

.
 i.e., the input 
\begin_inset Formula $\mathcal{I}_{v}$
\end_inset

 of layer 
\begin_inset Formula $\ell$
\end_inset

 is calculated as
\begin_inset Formula 
\[
\mathcal{I}_{v}=\sum_{u\in\mathcal{V}^{\left(\ell-1\right)}}w_{uv}\mathcal{Z}_{u}h\left(s_{uv}\right)
\]

\end_inset

where 
\begin_inset Formula $\mathcal{V}^{\left(\ell-1\right)}$
\end_inset

 is the set of neurons on the previous layer.
 The masking function 
\begin_inset Formula $h$
\end_inset

 takes the value of 
\begin_inset Formula $1$
\end_inset

 if 
\begin_inset Formula $s_{uv}$
\end_inset

 is in the top-
\begin_inset Formula $k\%$
\end_inset

 scores of layer 
\begin_inset Formula $\ell$
\end_inset

, and 
\begin_inset Formula $0$
\end_inset

 otherwise.
\end_layout

\begin_layout Standard
This poses a new problem, because the gradient of 
\begin_inset Formula $h$
\end_inset

 is always 
\begin_inset Formula $0$
\end_inset

.
 To deal with this, the straight-through gradient estimator 
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio2013"
literal "false"

\end_inset

 was proposed.
 This estimator simply skips 
\begin_inset Quotes eld
\end_inset

straight through
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $h$
\end_inset

 in the backwards pass, by ignoring its gradient and treating it as an identity
 operator.
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Subsection
Deep Inversion
\end_layout

\begin_layout Standard
First, we would like to emphasize that while we were working on this project,
 no implementation of Deep Inversion was publicly available, be it official
 or otherwise.
 Thus, even though our implementation tries to follow the description in
 the paper as closely as possible, we borrowed some details from Deep Dream
 and used our own judgement whenever something was vaguely described.
 Code can be found at:
\begin_inset Note Note
status open

\begin_layout Plain Layout
github url
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
First, we created a class 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

DeepInvert
\end_layout

\end_inset

 that holds the relevant information for the Deep Inversion environment.
 That is:
\end_layout

\begin_layout Itemize
The model
\end_layout

\begin_layout Itemize
The loss function
\end_layout

\begin_layout Itemize
The regularization function
\end_layout

\begin_layout Itemize
Mean and variance of the dataset the model was originally trained on, used
 for normalizing and clipping the input before passing it through the model.
\end_layout

\begin_layout Standard
The main method of our 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

DeepInvert
\end_layout

\end_inset

 class is 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

deepInvert
\end_layout

\end_inset

.
 This method applies the Deep Inversion algorithm.
 It receives:
\end_layout

\begin_layout Itemize
A batch of images (usually random Gaussian noise)
\end_layout

\begin_layout Itemize
Number of iterations to apply Deep Inversion
\end_layout

\begin_layout Itemize
Target labels to synthesize
\end_layout

\begin_layout Itemize
Hyper-parameters for the Adam optimizer
\end_layout

\begin_layout Itemize
Amount of jitter to use (defaults to 0)
\end_layout

\begin_layout Itemize
Probability of random flipping
\end_layout

\begin_layout Standard
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

deepInvert
\end_layout

\end_inset

 runs the following algorithm: 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
deepInvert
\end_layout

\end_inset


\end_layout

\begin_layout LyX-Code

\shape italic
# Normalize the input using the mean and
\end_layout

\begin_layout LyX-Code

\shape italic
# variance of the original training dataset
\end_layout

\begin_layout LyX-Code
input = preprocess(batch)
\end_layout

\begin_layout LyX-Code

\shape italic
# Create optimizer
\end_layout

\begin_layout LyX-Code
optimizer = Adam(params=[input],
\end_layout

\begin_layout LyX-Code
                            optimizer_hyperparameters)
\end_layout

\begin_layout LyX-Code

\series bold
for
\series default
 i = 1 ...
 iterations:
\end_layout

\begin_deeper
\begin_layout LyX-Code
apply_jitter_and_flip(input, jitter, flip)
\end_layout

\begin_layout LyX-Code
output = model.forward(input)
\end_layout

\begin_layout LyX-Code
optimizer.zero_grad()
\end_layout

\begin_layout LyX-Code
loss = loss_fn(output, target_labels) + reg_fn(input)
\end_layout

\begin_layout LyX-Code
loss.backward()
\end_layout

\begin_layout LyX-Code
optimizer.step()
\end_layout

\begin_layout LyX-Code
clip(input) 
\shape italic
# clips values out of range
\end_layout

\begin_layout LyX-Code
unjitter_and_unflip(input, jitter, flip)
\end_layout

\end_deeper
\begin_layout LyX-Code

\shape italic
# Denormalize, clip, and convert to PIL images
\end_layout

\begin_layout LyX-Code

\series bold
return
\series default
 postprocess(input)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Basically - 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

deepInvert
\end_layout

\end_inset

 initializes an optimizer to optimize the batch, performs the main loop
 for the specified number of iterations, and lastly saves the generated
 images.
 The main loop is similar to a regular training loop - forward pass, loss
 calculation, backward pass, and optimization step.
 The main differences are:
\end_layout

\begin_layout Itemize
We apply jitter before each iteration.
 As described in the paper, this means rolling the images horizontally and
 vertically by a random amount.
\end_layout

\begin_layout Itemize
The regularization function is applied on the input, rather than on the
 model's output.
 This is because generating images requires optimizing the input itself.
\end_layout

\begin_layout Itemize
We clip the input on each iteration.
 As we mentioned earlier this method was originally used in Google's implementat
ion of Deep Dream
\begin_inset Foot
status open

\begin_layout Plain Layout
https://github.com/google/deepdream
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Our implementation of course supports the use of CUDA, as well as the use
 of Automatic Mixed Precision as described in the original Deep Inversion
 paper.
\end_layout

\begin_layout Subsubsection
Feature Regularization
\end_layout

\begin_layout Standard
Perhaps the most important part of Deep Inversion, and the major contribution
 of the paper, is the 
\begin_inset Formula $\rfeature$
\end_inset

 regularization term.
\end_layout

\begin_layout Standard
At first, we had some trouble implementing it, because it requires collecting
 the output of the hidden layers of the network.
 After some research, we concluded that using 
\shape italic
hooks
\shape default
 will help implementing this.
 Our implementation of 
\begin_inset Formula $\rfeature$
\end_inset

 registers a hook function to all of the BatchNorm layers.
 This way, the hook function is called during the forward pass, i.e., every
 time before a BatchNorm layer's 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

forward
\end_layout

\end_inset

 is invoked.
\begin_inset Foot
status open

\begin_layout Plain Layout
https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_forward_pre_hook
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Initially, our hook function computed the batch-wise mean and variance of
 feature maps corresponding to the current layer, 
\begin_inset Formula $\mu_{l}\left(\hat{x}\right),\sigma_{l}^{2}\left(\hat{x}\right)$
\end_inset

, and stored them in a list.
 This list was later used for calculating the regularization term as it
 appeared in the paper 
\begin_inset CommandInset citation
LatexCommand cite
key "Yin2019"
literal "false"

\end_inset

:
\begin_inset Formula 
\begin{align*}
\rfeature & =\sum_{l}\left\Vert \mu_{l}\left(\hat{x}\right)-\text{BN}_{l}\left(\text{running\_mean}\right)\right\Vert _{2}\\
 & +\sum_{l}\left\Vert \sigma_{l}^{2}\left(\hat{x}\right)-\text{BN}_{l}\left(\text{running\_variance}\right)\right\Vert _{2}
\end{align*}

\end_inset

However, we noticed this was causing high GPU memory usages.
 Due to the additive nature of 
\begin_inset Formula $\rfeature$
\end_inset

, we were able to change our implementation to compute 
\begin_inset Formula $\rfeature$
\end_inset

 on the fly.
 Therefore, our hook function performed the following update
\begin_inset Formula 
\begin{align*}
\rfeature & =\rfeature\\
 & +\left\Vert \mu_{l}\left(\hat{x}\right)-\text{BN}_{l}\left(\text{running\_mean}\right)\right\Vert _{2}\\
 & +\left\Vert \sigma_{l}^{2}\left(\hat{x}\right)-\text{BN}_{l}\left(\text{running\_variance}\right)\right\Vert _{2}
\end{align*}

\end_inset

And of course, 
\begin_inset Formula $\rfeature$
\end_inset

 was initialized to 
\begin_inset Formula $0$
\end_inset

, and was also reset after each time the 
\begin_inset listings
inline true
status open

\begin_layout Plain Layout

forward
\end_layout

\end_inset

 function of the regularization was invoked.
 
\end_layout

\begin_layout Subsubsection
Argument Parser
\end_layout

\begin_layout Standard
To allow easy execution of experiments, we implemented an argument parser
 that allows running different configurations of Deep Inversion both programmati
cally and through the command line.
 The parser is rich in options, as can be seen by its help message: 
\begin_inset Note Note
status open

\begin_layout Plain Layout
fix the margins of this section
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

usage: main.py [-h] [--iterations I] [--early-stopping ES] [--batch-size
 B]
\end_layout

\begin_layout Plain Layout

               [--no-cuda] [--amp-mode AMP] [--seed S] [--lr LR]
\end_layout

\begin_layout Plain Layout

               [--scheduler-patience SP] [--targets T [T ...]] [--dataset DS]
\end_layout

\begin_layout Plain Layout

               [--model-name MN] [--jitter J] [--flip FLP] [--loss-fn LF]
\end_layout

\begin_layout Plain Layout

               [--temp TMP] [--reg-fn RF] [--a-tv ATV] [--a-l2 AL2] [--a-f
 AF]
\end_layout

\begin_layout Plain Layout

               [--output-dir OD]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Deep Inversion
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

optional arguments:
\end_layout

\begin_layout Plain Layout

  -h, --help            show this help message and exit
\end_layout

\begin_layout Plain Layout

  --iterations I        number of epochs to run Deep Inversion for (default:
\end_layout

\begin_layout Plain Layout

                        20000)
\end_layout

\begin_layout Plain Layout

  --early-stopping ES   percentage of iterations with no improvement to
 wait
\end_layout

\begin_layout Plain Layout

                        before early stopping (default: 15)
\end_layout

\begin_layout Plain Layout

  --batch-size B        number of images to generate in a batch (default:
 128)
\end_layout

\begin_layout Plain Layout

  --no-cuda             disable CUDA
\end_layout

\begin_layout Plain Layout

  --amp-mode AMP        Automatic Mixed Precision mode (default: O2)
\end_layout

\begin_layout Plain Layout

  --seed S              random seed (default: None)
\end_layout

\begin_layout Plain Layout

  --lr LR               learning rate (default: 0.2)
\end_layout

\begin_layout Plain Layout

  --scheduler-patience SP
\end_layout

\begin_layout Plain Layout

                        learning rate scheduler patience in percentage
\end_layout

\begin_layout Plain Layout

                        relative to the number of iterations (default: 5)
\end_layout

\begin_layout Plain Layout

  --targets T [T ...]   target classes for image synthesis, or -1 for
\end_layout

\begin_layout Plain Layout

                        randomization (default: -1)
\end_layout

\begin_layout Plain Layout

  --dataset DS          dataset to perform synthesis on (default: ImageNet)
\end_layout

\begin_layout Plain Layout

  --model-name MN       name of model to use for synthesis (default: ResNet50)
\end_layout

\begin_layout Plain Layout

  --jitter J            amount of jitter to apply on each iteration (default:
\end_layout

\begin_layout Plain Layout

                        30)
\end_layout

\begin_layout Plain Layout

  --flip FLP            horizontal flip probability (default: 0.5)
\end_layout

\begin_layout Plain Layout

  --loss-fn LF          loss function (default: CE)
\end_layout

\begin_layout Plain Layout

  --temp TMP            temperature value for CrossEntropyLoss (default:
 1)
\end_layout

\begin_layout Plain Layout

  --reg-fn RF           regularization function (default: DI)
\end_layout

\begin_layout Plain Layout

  --a-tv ATV            TV regularization factor (default: 8e-3)
\end_layout

\begin_layout Plain Layout

  --a-l2 AL2            l2-norm regularization factor (default: 1e-5)
\end_layout

\begin_layout Plain Layout

  --a-f AF              feature regularization factor (default: 1e-2)
\end_layout

\begin_layout Plain Layout

  --output-dir OD       directory for storing generated images (default:
\end_layout

\begin_layout Plain Layout

                        generated) 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Hidden Subnetworks
\end_layout

\begin_layout Standard
For the Hidden Subnetworks, we used the official PyTorch implementation
 given by the authors of the paper
\begin_inset Foot
status open

\begin_layout Plain Layout
https://github.com/allenai/hidden-networks
\end_layout

\end_inset

.
 Their implementation also includes 
\begin_inset Quotes eld
\end_inset

pretrained
\begin_inset Quotes erd
\end_inset

 subnetworks, i.e., well performing subnetworks of randomly initialized networks
 which were found using their method.
\end_layout

\begin_layout Subsection
Deep Inversion of Hidden Subnetworks
\end_layout

\begin_layout Standard
Applying Deep Inversion on a Hidden Subnetwork proved to be a bit challenging.
 This is because the implementation of Hidden Subnetworks was designed to
 work solely using the command line, and heavily relied on global variables.
 Thus, trying to extract a pretrained subnetwork from the original implementatio
n required us to use hacky methods, such as modifying their global variables
 within our code, and fooling their argument parser to receive arguments
 programmatically (instead of through the command line).
\end_layout

\begin_layout Standard
Once we managed to extract a pretrained subnetwork, applying Deep Inversion
 on it was pretty straight forward.
 We just set the model used by Deep Inversion to be the subnetwork, and
 then executed it as usual.
 The modularity of our implementation is what allowed this simplicity.
\end_layout

\begin_layout Section
Experiments & Results
\end_layout

\begin_layout Subsection
Hyper-Parameter Tuning for DeepInversion
\begin_inset CommandInset label
LatexCommand label
name "subsec:Hyper-Parameter-Tuning-for"

\end_inset


\end_layout

\begin_layout Standard
After completing the implementation of DeepInversion we moved on to test
 the method with the hyper-parameters mentioned in 
\begin_inset CommandInset citation
LatexCommand cite
key "Yin2019"
literal "false"

\end_inset

.
 Yin et al.
 conducted several experiments to test the method's ability to reconstruct
 images from models that were pretrained on the CIFAR-10 and ImageNet datasets.
 For ImageNet, the authors claim that they were able to synthesize quality
 images using the publicly available ResNet-50 model from PyTorch.
 They used an Adam optimizer 
\begin_inset CommandInset citation
LatexCommand cite
key "Kingma2014"
literal "false"

\end_inset

 with a learning-rate of 
\begin_inset Formula $0.05$
\end_inset

, and the following hyper-parameters:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\afeature=1\cdotp10^{-2},\atv=1\cdotp10^{-4},\anorm=1\cdotp10^{-2}
\]

\end_inset


\end_layout

\begin_layout Standard
Unfortunately we weren't able to reproduce the paper's results using the
 above hyper-parameters.
\end_layout

\begin_layout Standard
These results indicate that DeepInversion is very sensitive to the hyper-paramet
ers being used, and that even the slightest change in the way the method
 is implemented might affect these parameters.
 This unfortunately made us realize that we had to tune our implementation
 by ourselves, which eventually proved to be a 
\bar under
very
\bar default
 difficult and time-demanding task.
\end_layout

\begin_layout Standard
To do so we implemented a grid-search algorithm which tests the method with
 different combinations of hyper-parameter values (for the parameters not
 described above we used the values that were mentioned in the paper).
 Then, to evaluate the synthesized images from each combination we used
 the Inception Score 
\begin_inset CommandInset citation
LatexCommand cite
key "Salimans2016"
literal "false"

\end_inset

, a popular method to evaluate the image outputs of GANs, which was empirically
 found to correlate well with human evaluation.
 
\end_layout

\begin_layout Standard
After running the grid-search algorithm for several days and testing a decent
 number of combinations the best parameters found for our implementation
 were the following: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\afeature=1\cdotp10^{-2},\atv=8\cdotp10^{-3},\anorm=1\cdotp10^{-5}
\]

\end_inset


\end_layout

\begin_layout Standard
It should be noted, however, that the computational limitations imposed
 on us by the faculty's server (a single shared NVIDIA Titan X 12GB) did
 not allow us to match the batch size that was used in the experiments in
 the paper, or to run the grid-search algorithm as extensively as we would
 have liked.
 
\end_layout

\begin_layout Standard
While the authors of DeepInversion used a batch size of 
\begin_inset Formula $1,216$
\end_inset

 using 8 NVIDIA V100 GPUs, our GPU's memory constraints limited us to test
 the method with a batch size of only 60-80 (or slightly more using AMP).
 We hypothesize that using a larger batch size might lead to better results,
 since a bigger batch would be able to better fit to the distribution imposed
 by 
\begin_inset Formula $\rfeature$
\end_inset

 during synthesis.
 
\end_layout

\begin_layout Standard
Also, the results from our grid-search experiment empirically showed that
 the hyper-parameter to Inception Score function we were trying to optimize
 was 
\bar under
highly non-convex
\bar default
.
 This, along with the fact that our model is highly sensitive to hyper-parameter
s, makes us believe that a significantly better set of hyper-parameters
 can be found for our implementation.
 However, that would require running the tuning algorithm on a grid with
 a much finer parameter resolution, and that could unfortunately take weeks
 using our limited hardware.
\end_layout

\begin_layout Standard
Below we present some of the images synthesized using our implementation
 with a ResNet-50 model pretrained on ImageNet:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Figures/Experiments/DI Image Examples.png
	scale 40
	BoundingBox 0bp 0bp 851bp 573bp

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Labeled samples from applying Deep Inversion on ResNet-50 pretrained on
 ImageNet
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We would like to emphasize, however, that not all of the images synthesized
 using our implementation with the above hyper-parameters turned out to
 be of high quality.
 We state again that further tuning is still required to obtain stable high-qual
ity batches.
 
\end_layout

\begin_layout Subsection
Evaluation of the Synthesized Images 
\begin_inset CommandInset label
LatexCommand label
name "Experiment:Evalute-synthesized"

\end_inset


\end_layout

\begin_layout Standard
In this section we propose a way to evaluate the quality of the images synthesiz
ed using our implementation of DeepInversion.
\end_layout

\begin_layout Standard
We hand-picked 10 categories from ImageNet, and for each category we synthesized
 500 images using our implementation with a pretrained ResNet-50.
\end_layout

\begin_layout Standard
Then, we used a pretrained ResNet-152 (which has a 
\begin_inset Formula $78.31\%/94.06\%$
\end_inset

 top-1/top-5 accuracy respectively on ImageNet) to classify the synthesized
 images.
\end_layout

\begin_layout Standard
The model achieved 
\begin_inset Formula $97.02\%/99.32\%$
\end_inset

 top-1/top-5 accuracy respectively on our synthesized dataset.
\end_layout

\begin_layout Subsection
Knowledge Distillation Using the Synthesized Images
\end_layout

\begin_layout Standard
One of the most intriguing utilizations of DeepInversion is knowledge distillati
on.
 Yin et al.
 claim in 
\begin_inset CommandInset citation
LatexCommand cite
key "Yin2019"
literal "false"

\end_inset

 that the method enables transferring knowledge from a pretrained teacher
 to a randomly initialized student CNN without needing the original dataset
 the teacher was trained on.
\end_layout

\begin_layout Standard
To test the knowledge distillation potential of our synthesized images,
 we used the dataset we synthesized in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Experiment:Evalute-synthesized"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (5,000 images for 10 hand-picked classes) to train a randomly initialized
 'student' ResNet-50 from scratch.
 For this experiment we used the PyTorch implementation of ResNet-50, but
 since the model was originally designed for ImageNet, we had to replace
 its fully connected layer with a layer which fits our smaller dataset.
\end_layout

\begin_layout Standard
After training we evaluated the trained model using a test set of 500 images
 (50 images per class) originated from the validation set of ImageNet.
 Note that the pretrained model we used to synthesize our dataset with was
 
\bar under
not
\bar default
 trained on these images.
 
\end_layout

\begin_layout Standard
Unfortunately, as we stated in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Hyper-Parameter-Tuning-for"
plural "false"
caps "false"
noprefix "false"

\end_inset

 our hardware limitations made it very difficult for us to tune our DI model,
 and thus we suspected that the quality of the images in our synthesized
 dataset would not be high enough for knowledge distillation purposes.
 
\end_layout

\begin_layout Standard
Testing the above student model on the test set validated our suspicions,
 as it was only able to achieve 
\begin_inset Formula $38.8\%/62.4\%$
\end_inset

 top-1/top-3 accuracy respectively on the set.
\end_layout

\begin_layout Standard
Still, we believe that these results show the potential of our implementation,
 and we think that they can be much improved by tuning our DI model even
 further while using a bigger batch size during synthesis (both would require
 better hardware, however).
\end_layout

\begin_layout Subsection
Deep Inversion of Hidden Subnetworks
\end_layout

\begin_layout Standard
After experimenting on our implementation of Deep Inversion, we finally
 got to run it on a well performing subnetwork of a randomly initialized
 ResNet-50 that was extracted using the edge-popup algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Ramanujan2019"
literal "false"

\end_inset

.
 Our results are as follows:
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Figures/Experiments/DI + HiddenSubnets.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Labeled samples from applying Deep Inversion on a Hidden Subnetwork
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As shown above, the results resemble colored noise and are not distinguishable
 at all.
 We actually expected this experiment to fail due to one major reason: The
 hidden subnetworks extracted using the edge-popup algorithm do not contain
 the BatchNorm statistics mentioned in the Feature Regularization in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "Method:Regularizations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We can see however that the other regularization terms did have some effect
 on the output, for example, the Total Variation regularization caused nearby
 pixels to have similar color.
 
\end_layout

\begin_layout Standard
One could then ask why our results do not resemble those of Deep Dream,
 then? We believe the reason for that is that while the loss function used
 in Deep Dream is usually the raw output of some layer (or the class scores,
 in case of the last layer), In Deep Inversion on the other hand the loss
 function is the Cross Entropy loss.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Simonyan2014"
literal "false"

\end_inset

 it is claimed that on top of maximizing the probability that the image
 will belong to a specific label, using the Cross Entropy loss 
\bar under
also
\bar default
 tries to minimize the probability of it belonging to any of the other labels,
 which could potentially insert unwanted noise into the image.
 Furthermore, we believe that using Deep Inversion without 
\begin_inset Formula $\rfeature$
\end_inset

 (which is effectively the case here) would require re-tuning the rest of
 the hyper-parameters in order to maximize the quality of the synthesized
 images.
 However, both suggestions (changing the loss function and re-tuning the
 hyper-parameters) are outside the scope of the project.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
In this project we have presented the first public implementation of DeepInversi
on, a method which allows reconstructing high-quality class-conditional
 images from a pretrained CNN using the hidden data stored in its batch
 normalization layers.
 
\end_layout

\begin_layout Standard
We believe that our results demonstrate the potential of the method for
 purposes such as knowledge distillation, but have also concluded that in
 order to unlock this full potential one would first have to further tune
 the model - a very challenging and time demanding task.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "Project"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
